$hive进入hive shell模式，可以进行如下操作。
操作生成的数据库、表文件可以在hadoop 网页界面里的/user/hive/warehouse目录下可以看到。

1、在hive里创建一个表，定义表的属性，以','为分隔符。
create table one(first String,second String,third int) Row format delimited fields terminated by ',';

2、装载本地系统文件到表one
LOAD DATA LOCAL INPATH '/usr/dtest.txt' OVERWRITE INTO TABLE one;

3、查询表（支持count,sum函数，本人验证了）
select * from one;

4、装载dfs系统文件到表（注意load语句不能有local,与本地文件系统进行区分）
LOAD DATA INPATH '/user/edisonchou/events-.1493946412768' OVERWRITE INTO TABLE one;

5、Hive这次启动目录
/usr/local/hadoop-2.8.0/tmp/dfs/name

6、执行.sql文件
source 路径/文件名.sql

7、查看指定表的结构
desc 表名

8、to_date函数将yyyy-mm-dd hh:mm:ss格式日期转换为yyyy-mm-dd格式
hive> select * from pageclose where to_date(closetime) = '2017-05-05';

9、可以这么嵌套查询
hive> select * from (select * from temptotal) e;

10、创建分区表
	CREATE TABLE userlogin (structname string,flowId string,employeeId string,systemId string,loginTime timestamp,browser string,
  os string,resolving string) PARTITIONED BY (inputdate string) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' 
  lines terminated by '\n';
  
  	添加分区（本人有疑意）
	ALTER TABLE userlogin  ADD PARTITION(inputdate='2017-05-13');
11、load分区表数据
load data local inpath '/root/example.txt' overwrite into table userlogin partition (inputdate='2017-05-16')


12、use tlog;
select concat(systemid,","), distinct concat(employeeid,","),to_date(from_unixtime(unix_timestamp(),'yyyy-MM-dd HH:mm:ss')) from userlogin where to_date(inputdate) = to_date

(from_unixtime(unix_timestamp(),'yyyy-MM-dd HH:mm:ss')) group by systemid;

13 group by 分组查询可以不用聚合函数，但是必须与group by 的条件字段一致。可以用两个字段联合分组。
例如：select systemid,employeeid from userlogin group by systemid,employeeid; 

14 hive求当前日期星期几
select pmod(datediff('2017-05-14','2017-01-01'),7);       周一至周日 (0-6)

select pmod(datediff('2017-01-01','2017-01-02'),7) + 1;   周一至周日 （1-7）;








